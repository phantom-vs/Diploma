{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db2bde50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bca09b",
   "metadata": {},
   "source": [
    "ЗАГРУЗКА И ПРЕПРОЦЕССИНГ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1e5f7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FA0183FS.EEG\n",
      "Found 21E file, reading channel names.\n",
      "Reading header from /home/jovyan/vspyatochkin/diploma/00000000/NKT/EEG2100/FA0183FS.EEG\n",
      "Reading 0 ... 19248999  =      0.000 ... 38497.998 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_605090/2305222571.py:1: RuntimeWarning: No PNT file exists. Metadata will be blank\n",
      "  raw = mne.io.read_raw_nihon('./00000000/NKT/EEG2100/FA0183FS.EEG', preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found LOG file, reading events.\n",
      "EEG channel type selected for re-referencing\n",
      "Creating RawArray with float64 data, n_channels=18, n_times=19249000\n",
      "    Range : 0 ... 19248999 =      0.000 ... 38497.998 secs\n",
      "Ready.\n",
      "Added the following bipolar channels:\n",
      "Fp1-F3, Fp2-F4, F3-C3, F4-C4, C3-P3, C4-P4, P3-O1, P4-O2, Fp1-F7, Fp2-F8, F7-T3, F8-T4, T3-T5, T4-T6, T5-O1, T6-O2, Fz-Cz, Cz-Pz\n",
      "Данные загружены: (18, 19249000)\n",
      "Частота дискретизации: 500.0 Гц\n"
     ]
    }
   ],
   "source": [
    "raw = mne.io.read_raw_nihon('./00000000/NKT/EEG2100/FA0183FS.EEG', preload=True)\n",
    "sfreq = raw.info['sfreq']  # 500 Гц\n",
    "\n",
    "# Создание биполярного монтажа (как в DotNetChecker)\n",
    "anodes = ['Fp1', 'Fp2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4',\n",
    "          'Fp1', 'Fp2', 'F7', 'F8', 'T3', 'T4', 'T5', 'T6',\n",
    "          'Fz', 'Cz']\n",
    "cathodes = ['F3', 'F4', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2',\n",
    "            'F7', 'F8', 'T3', 'T4', 'T5', 'T6', 'O1', 'O2',\n",
    "            'Cz', 'Pz']\n",
    "ch_names = [f'{a}-{c}' for a, c in zip(anodes, cathodes)]\n",
    "\n",
    "raw_bip = mne.set_bipolar_reference(\n",
    "    raw, anode=anodes, cathode=cathodes, \n",
    "    ch_name=ch_names, drop_refs=True, copy=True\n",
    ")\n",
    "raw_bip.pick(ch_names)  # Только нужные 18 каналов\n",
    "\n",
    "# Получаем данные\n",
    "data = raw_bip.get_data()  # [18, n_times]\n",
    "print(f\"Данные загружены: {data.shape}\")\n",
    "print(f\"Частота дискретизации: {sfreq} Гц\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eef92f",
   "metadata": {},
   "source": [
    "СОЗДАНИЕ МЕТОК ДЛЯ КЛАССИФИКАЦИИ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6818677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные для классификации: (18, 1080000)\n",
      "Длительность: 36.0 минут\n",
      "Всего событий ДЭРД: 387\n",
      "Загружено 387 событий\n",
      "Средняя длительность ДЭРД: 0.326 сек\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('depd_intervals.csv')  # channel, start_seconds, end_seconds\n",
    "\n",
    "# Размечены только первые 36 минут\n",
    "ANNOTATED_DURATION = 36 * 60\n",
    "annotated_samples = int(ANNOTATED_DURATION * sfreq)\n",
    "data = data[:, :annotated_samples]  # [18, 1,080,000]\n",
    "n_channels, n_times = data.shape\n",
    "\n",
    "print(f\"Данные для классификации: {data.shape}\")\n",
    "print(f\"Длительность: {n_times/sfreq/60:.1f} минут\")\n",
    "print(f\"Всего событий ДЭРД: {len(df)}\")\n",
    "\n",
    "# Создаем список событий для быстрого поиска\n",
    "events = []\n",
    "for _, row in df.iterrows():\n",
    "    ch = int(row['channel']) - 1  # Каналы с 1 → с 0\n",
    "    start_sample = int(row['start_seconds'] * sfreq)\n",
    "    end_sample = int(row['end_seconds'] * sfreq)\n",
    "    \n",
    "    if 0 <= ch < n_channels and start_sample < n_times:\n",
    "        end_sample = min(end_sample, n_times)\n",
    "        events.append({\n",
    "            'channel': ch,\n",
    "            'start': start_sample,\n",
    "            'end': end_sample,\n",
    "            'duration': end_sample - start_sample\n",
    "        })\n",
    "\n",
    "print(f\"Загружено {len(events)} событий\")\n",
    "if events:\n",
    "    avg_duration = np.mean([e['duration'] for e in events]) / sfreq\n",
    "    print(f\"Средняя длительность ДЭРД: {avg_duration:.3f} сек\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af9dd88",
   "metadata": {},
   "source": [
    "СОЗДАНИЕ ОКОН"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ed09bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Создано окон: 4317\n",
      "Размер окна: (18, 1000)\n",
      "Процент окон с ДЭРД: 1.99%\n",
      "Окон с ДЭРД: 732 из 4317\n"
     ]
    }
   ],
   "source": [
    "def create_windows_for_classification(data, events, window_sec=2.0, overlap_sec=1.5):\n",
    "    \"\"\"Создает короткие окна для классификации\"\"\"\n",
    "    window_samples = int(window_sec * sfreq)  # 1000 отсчетов (2 сек)\n",
    "    step_samples = int((window_sec - overlap_sec) * sfreq)  # 250 отсчетов (0.5 сек)\n",
    "    \n",
    "    windows_data = []\n",
    "    windows_labels = []  # Метки: [0/1 для каждого канала]\n",
    "    \n",
    "    n_samples = data.shape[1]\n",
    "    \n",
    "    # Создаем хэш-таблицу событий для быстрого поиска\n",
    "    # Ключ: (start_sample, channel), Значение: end_sample\n",
    "    event_dict = {}\n",
    "    for event in events:\n",
    "        key = (event['start'], event['channel'])\n",
    "        event_dict[key] = event['end']\n",
    "    \n",
    "    for start in range(0, n_samples - window_samples + 1, step_samples):\n",
    "        end = start + window_samples\n",
    "        \n",
    "        window_data = data[:, start:end]  # [18, 1000]\n",
    "        \n",
    "        # Метка: есть ли ДЭРД в этом окне для каждого канала\n",
    "        label = np.zeros(n_channels, dtype=float)\n",
    "        \n",
    "        # Проверяем для каждого канала\n",
    "        for ch in range(n_channels):\n",
    "            # Ищем события на этом канале, которые пересекаются с окном\n",
    "            has_depd = False\n",
    "            for event_start, event_ch in event_dict.keys():\n",
    "                if event_ch == ch:  # Событие на этом канале\n",
    "                    event_end = event_dict[(event_start, ch)]\n",
    "                    # Проверяем пересечение события с окном\n",
    "                    if not (event_end <= start or event_start >= end):\n",
    "                        has_depd = True\n",
    "                        break\n",
    "            \n",
    "            label[ch] = 1.0 if has_depd else 0.0\n",
    "        \n",
    "        windows_data.append(window_data)\n",
    "        windows_labels.append(label)\n",
    "    \n",
    "    return np.array(windows_data), np.array(windows_labels)\n",
    "\n",
    "# Создаем окна 2 секунды\n",
    "windows_data, windows_labels = create_windows_for_classification(\n",
    "    data, events, window_sec=2.0, overlap_sec=1.5\n",
    ")\n",
    "\n",
    "print(f\"\\nСоздано окон: {len(windows_data)}\")\n",
    "print(f\"Размер окна: {windows_data[0].shape}\")\n",
    "print(f\"Процент окон с ДЭРД: {100 * windows_labels.sum() / windows_labels.size:.2f}%\")\n",
    "print(f\"Окон с ДЭРД: {(windows_labels.sum(axis=1) > 0).sum()} из {len(windows_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b742c44b",
   "metadata": {},
   "source": [
    "БАЛАНСИРОВКА"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36ee2143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Окон с ДЭРД: 732\n",
      "Окон без ДЭРД: 3585\n",
      "После балансировки:\n",
      "Всего окон: 1464\n",
      "Окон с ДЭРД: 732 (50.0%)\n"
     ]
    }
   ],
   "source": [
    "positive_indices = np.where(windows_labels.sum(axis=1) > 0)[0]\n",
    "negative_indices = np.where(windows_labels.sum(axis=1) == 0)[0]\n",
    "\n",
    "print(f\"Окон с ДЭРД: {len(positive_indices)}\")\n",
    "print(f\"Окон без ДЭРД: {len(negative_indices)}\")\n",
    "\n",
    "# Берем все положительные и столько же отрицательных\n",
    "selected_negatives = np.random.choice(\n",
    "    negative_indices, \n",
    "    size=min(len(positive_indices), len(negative_indices)), \n",
    "    replace=False\n",
    ")\n",
    "\n",
    "balanced_indices = np.concatenate([positive_indices, selected_negatives])\n",
    "np.random.shuffle(balanced_indices)\n",
    "\n",
    "balanced_data = windows_data[balanced_indices]\n",
    "balanced_labels = windows_labels[balanced_indices]\n",
    "\n",
    "print(f\"После балансировки:\")\n",
    "print(f\"Всего окон: {len(balanced_data)}\")\n",
    "print(f\"Окон с ДЭРД: {len(positive_indices)} ({len(positive_indices)/len(balanced_data):.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0b7ab1",
   "metadata": {},
   "source": [
    "РАЗДЕЛЕНИЕ НА TRAIN/VAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1607389b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: 1171 окон\n",
      "Val: 293 окон\n"
     ]
    }
   ],
   "source": [
    "TRAIN_RATIO = 0.8\n",
    "split_idx = int(len(balanced_data) * TRAIN_RATIO)\n",
    "\n",
    "split_idx = int(len(balanced_data) * 0.8)\n",
    "train_data = balanced_data[:split_idx]\n",
    "train_labels = balanced_labels[:split_idx]\n",
    "val_data = balanced_data[split_idx:]\n",
    "val_labels = balanced_labels[split_idx:]\n",
    "\n",
    "print(f\"Окна для тренировки: {len(train_data)}\")\n",
    "print(f\"Окна для валидации: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9ddc44",
   "metadata": {},
   "source": [
    "АРХИТЕКТУРА МОДЕЛИ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f934f389",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEPDClassifier(nn.Module):\n",
    "    \"\"\"Классификатор: есть ли ДЭРД в окне для каждого канала\"\"\"\n",
    "    def __init__(self, n_channels=18, window_size=15000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Признаки из каждого канала\n",
    "        self.channel_features = nn.Sequential(\n",
    "            nn.Conv1d(1, 16, kernel_size=101, padding=50),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(4),\n",
    "            \n",
    "            nn.Conv1d(16, 32, kernel_size=51, padding=25),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(4),\n",
    "            \n",
    "            nn.Conv1d(32, 64, kernel_size=25, padding=12),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)  # [B, 64, 1]\n",
    "        )\n",
    "        \n",
    "        # 2. Объединение признаков со всех каналов\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * n_channels, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(64, n_channels)  # По одному выходу на канал\n",
    "        )\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [B, 18, 15000]\n",
    "        batch_size, n_channels, window_size = x.shape\n",
    "        \n",
    "        # Обрабатываем каждый канал отдельно\n",
    "        channel_features = []\n",
    "        for ch in range(n_channels):\n",
    "            # Берем один канал: [B, 1, 15000]\n",
    "            channel_data = x[:, ch:ch+1, :]\n",
    "            features = self.channel_features(channel_data)  # [B, 64, 1]\n",
    "            features = features.view(batch_size, -1)  # [B, 64]\n",
    "            channel_features.append(features)\n",
    "        \n",
    "        # Объединяем: [B, 64*18]\n",
    "        combined = torch.cat(channel_features, dim=1)\n",
    "        \n",
    "        # Классификация\n",
    "        out = self.fc(combined)  # [B, 18]\n",
    "        return self.sigmoid(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48862244",
   "metadata": {},
   "source": [
    "ДАТАЛОАДЕРЫ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b24b57a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 32\n",
      "Батчей в train: 37\n",
      "Батчей в val: 10\n"
     ]
    }
   ],
   "source": [
    "class EEGClassificationDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.FloatTensor(data)   # [B, 18, 1000]\n",
    "        self.labels = torch.FloatTensor(labels)  # [B, 18]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = EEGClassificationDataset(train_data, train_labels)\n",
    "val_dataset = EEGClassificationDataset(val_data, val_labels)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Батчей в train: {len(train_loader)}\")\n",
    "print(f\"Батчей в val: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7418589a",
   "metadata": {},
   "source": [
    "ТРЕНИРОВКА"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bdec59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Устройство: cuda\n",
      "Epoch   0 | Train Loss: 0.2958 | Val Loss: 0.1435 | F1: 0.0000 | Prec: 0.0000 | Rec: 0.0000\n",
      "Epoch   1 | Train Loss: 0.1715\n",
      "Epoch   2 | Train Loss: 0.1590 | Val Loss: 0.1431 | F1: 0.0066 | Prec: 0.2500 | Rec: 0.0033\n",
      "Epoch   3 | Train Loss: 0.1516\n",
      "Epoch   4 | Train Loss: 0.1459 | Val Loss: 0.3071 | F1: 0.0000 | Prec: 0.0000 | Rec: 0.0000\n",
      "Epoch   5 | Train Loss: 0.1389\n",
      "Epoch   6 | Train Loss: 0.1287 | Val Loss: 0.9164 | F1: 0.0000 | Prec: 0.0000 | Rec: 0.0000\n",
      "Epoch   7 | Train Loss: 0.1240\n",
      "Epoch   8 | Train Loss: 0.1178 | Val Loss: 0.8423 | F1: 0.0197 | Prec: 0.6000 | Rec: 0.0100\n",
      "Epoch   9 | Train Loss: 0.1142\n",
      "Epoch  10 | Train Loss: 0.1104 | Val Loss: 0.1547 | F1: 0.0195 | Prec: 0.3750 | Rec: 0.0100\n",
      "Epoch  11 | Train Loss: 0.1086\n",
      "Epoch  12 | Train Loss: 0.1092 | Val Loss: 0.2595 | F1: 0.0198 | Prec: 0.7500 | Rec: 0.0100\n",
      "Epoch  13 | Train Loss: 0.1053\n",
      "Epoch  14 | Train Loss: 0.0994 | Val Loss: 0.3545 | F1: 0.2043 | Prec: 0.5205 | Rec: 0.1271\n",
      "Epoch  15 | Train Loss: 0.0970\n",
      "Epoch  16 | Train Loss: 0.0937 | Val Loss: 0.2870 | F1: 0.2784 | Prec: 0.2862 | Rec: 0.2709\n",
      "Epoch  17 | Train Loss: 0.0956\n",
      "Epoch  18 | Train Loss: 0.0878 | Val Loss: 0.7874 | F1: 0.0066 | Prec: 0.3333 | Rec: 0.0033\n",
      "Epoch  19 | Train Loss: 0.0846\n",
      "Epoch  20 | Train Loss: 0.0806 | Val Loss: 1.8205 | F1: 0.3148 | Prec: 0.2380 | Rec: 0.4649\n",
      "Epoch  21 | Train Loss: 0.0782\n",
      "Epoch  22 | Train Loss: 0.0760 | Val Loss: 0.2378 | F1: 0.3575 | Prec: 0.3447 | Rec: 0.3712\n",
      "Epoch  23 | Train Loss: 0.0706\n",
      "Epoch  24 | Train Loss: 0.0719 | Val Loss: 0.2842 | F1: 0.3379 | Prec: 0.2556 | Rec: 0.4983\n",
      "Epoch  25 | Train Loss: 0.0660\n",
      "Epoch  26 | Train Loss: 0.0662 | Val Loss: 0.3420 | F1: 0.3292 | Prec: 0.2624 | Rec: 0.4415\n",
      "Epoch  27 | Train Loss: 0.0691\n",
      "Epoch  28 | Train Loss: 0.0627 | Val Loss: 2.1274 | F1: 0.3265 | Prec: 0.3357 | Rec: 0.3177\n",
      "Epoch  29 | Train Loss: 0.0656\n",
      "Epoch  30 | Train Loss: 0.0612 | Val Loss: 0.1801 | F1: 0.0260 | Prec: 0.4444 | Rec: 0.0134\n",
      "Epoch  31 | Train Loss: 0.0597\n",
      "Epoch  32 | Train Loss: 0.0628 | Val Loss: 0.2717 | F1: 0.4105 | Prec: 0.3114 | Rec: 0.6020\n",
      "Epoch  33 | Train Loss: 0.0577\n",
      "Epoch  34 | Train Loss: 0.0564 | Val Loss: 0.1614 | F1: 0.3255 | Prec: 0.2898 | Rec: 0.3712\n",
      "Epoch  35 | Train Loss: 0.0535\n",
      "Epoch  36 | Train Loss: 0.0539 | Val Loss: 0.3143 | F1: 0.3753 | Prec: 0.2852 | Rec: 0.5485\n",
      "Epoch  37 | Train Loss: 0.0530\n",
      "Epoch  38 | Train Loss: 0.0556 | Val Loss: 0.2346 | F1: 0.2306 | Prec: 0.4600 | Rec: 0.1538\n",
      "Epoch  39 | Train Loss: 0.0498\n",
      "Epoch  40 | Train Loss: 0.0468 | Val Loss: 0.7649 | F1: 0.0000 | Prec: 0.0000 | Rec: 0.0000\n",
      "Epoch  41 | Train Loss: 0.0470\n",
      "Epoch  42 | Train Loss: 0.0473 | Val Loss: 0.2304 | F1: 0.0264 | Prec: 1.0000 | Rec: 0.0134\n",
      "Epoch  43 | Train Loss: 0.0447\n",
      "Epoch  44 | Train Loss: 0.0484 | Val Loss: 0.3036 | F1: 0.3316 | Prec: 0.3392 | Rec: 0.3244\n",
      "Epoch  45 | Train Loss: 0.0440\n",
      "Epoch  46 | Train Loss: 0.0472 | Val Loss: 0.1437 | F1: 0.1860 | Prec: 0.7111 | Rec: 0.1070\n",
      "Epoch  47 | Train Loss: 0.0457\n",
      "Epoch  48 | Train Loss: 0.0381 | Val Loss: 0.5606 | F1: 0.0000 | Prec: 0.0000 | Rec: 0.0000\n",
      "Epoch  49 | Train Loss: 0.0387\n",
      "Epoch  50 | Train Loss: 0.0425 | Val Loss: 0.2397 | F1: 0.0133 | Prec: 1.0000 | Rec: 0.0067\n",
      "Epoch  51 | Train Loss: 0.0436\n",
      "Epoch  52 | Train Loss: 0.0389 | Val Loss: 0.1872 | F1: 0.0326 | Prec: 0.6250 | Rec: 0.0167\n",
      "Ранняя остановка на эпохе 52\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Устройство: {device}\")\n",
    "\n",
    "model = DEPDClassifier(n_channels=18).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "criterion = nn.BCELoss()  # Самый простой для начала\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Функции для тренировки и валидации\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_data, batch_mask in loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        batch_mask = batch_mask.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_data)\n",
    "        loss = criterion(outputs, batch_mask)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in loader:\n",
    "            batch_data = batch_data.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            \n",
    "            outputs = model(batch_data)\n",
    "            val_loss += criterion(outputs, batch_labels).item()\n",
    "            \n",
    "            pred = (outputs > 0.5).float()\n",
    "            all_preds.append(pred.cpu())\n",
    "            all_targets.append(batch_labels.cpu())\n",
    "    \n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_targets = torch.cat(all_targets)\n",
    "    \n",
    "    # Вычисляем F1-score\n",
    "    tp = (all_preds * all_targets).sum().item()\n",
    "    fp = (all_preds * (1 - all_targets)).sum().item()\n",
    "    fn = ((1 - all_preds) * all_targets).sum().item()\n",
    "    \n",
    "    precision = tp / (tp + fp + 1e-6)\n",
    "    recall = tp / (tp + fn + 1e-6)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-6) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return val_loss / len(loader), f1, precision, recall\n",
    "\n",
    "\n",
    "# Тренировочный цикл\n",
    "best_f1 = 0\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(100):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        val_loss, f1, precision, recall = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | F1: {f1:.4f} | Prec: {precision:.4f} | Rec: {recall:.4f}\")\n",
    "    \n",
    "        # Ранняя остановка\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Ранняя остановка на эпохе {epoch}\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b77ecd1",
   "metadata": {},
   "source": [
    "ТЕСТИРОВАНИЕ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b07d6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Тест на 294 примерах:\n",
      "Accuracy: 0.9020\n",
      "Precision: 0.3114\n",
      "Recall: 0.6020\n",
      "F1-score: 0.4105\n",
      "\n",
      "Первый пример:\n",
      "Истинные метки: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Предсказания:   [0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "Вероятности:    [0.182 0.    0.83  0.    0.    0.    0.    0.    0.808 0.    0.    0.\n",
      " 0.039 0.    0.001 0.    0.    0.   ]\n",
      "В этом окне нет ДЭРД\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('best_model.pth'):\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Предсказание на нескольких примерах\n",
    "with torch.no_grad():\n",
    "    # Берем несколько примеров из валидации\n",
    "    n_test = 294\n",
    "    test_samples = val_data[:n_test]  # [5, 18, 1000]\n",
    "    test_labels = val_labels[:n_test]  # [5, 18]\n",
    "    \n",
    "    test_tensor = torch.FloatTensor(test_samples).to(device)\n",
    "    predictions = model(test_tensor)\n",
    "    \n",
    "    predictions_np = predictions.cpu().numpy()  # [5, 18]\n",
    "    true_labels_np = test_labels  # [5, 18]\n",
    "    \n",
    "    # Вычисляем метрики\n",
    "    pred_binary = (predictions_np > 0.5).astype(float)\n",
    "    \n",
    "    # Accuracy по каналам\n",
    "    correct = (pred_binary == true_labels_np).mean()\n",
    "    \n",
    "    # F1-score для положительного класса\n",
    "    tp = (pred_binary * true_labels_np).sum()\n",
    "    fp = (pred_binary * (1 - true_labels_np)).sum()\n",
    "    fn = ((1 - pred_binary) * true_labels_np).sum()\n",
    "    \n",
    "    precision = tp / (tp + fp + 1e-6)\n",
    "    recall = tp / (tp + fn + 1e-6)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-6) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nТест на {n_test} примерах:\")\n",
    "    print(f\"Accuracy: {correct:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    \n",
    "    # Детальная информация по первому примеру\n",
    "    print(f\"\\nПервый пример:\")\n",
    "    print(f\"Истинные метки: {true_labels_np[0].astype(int)}\")\n",
    "    print(f\"Предсказания:   {pred_binary[0].astype(int)}\")\n",
    "    print(f\"Вероятности:    {predictions_np[0].round(3)}\")\n",
    "    \n",
    "    # Проверяем, на каких каналах были ДЭРД\n",
    "    depd_channels = np.where(true_labels_np[0] > 0.5)[0]\n",
    "    if len(depd_channels) > 0:\n",
    "        print(f\"Каналы с ДЭРД: {[ch+1 for ch in depd_channels]}\")\n",
    "    else:\n",
    "        print(\"В этом окне нет ДЭРД\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
